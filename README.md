# Apache Hadoop & Apache Spark 

 Assignment 1 -- MapReduce and Spark Analysis

**Name:** Arpita Kundu\
**Roll No:** M25DE1004

------------------------------------------------------------------------

### üìå Project Overview

This assignment demonstrates practical implementation of:

-   Apache Hadoop MapReduce (WordCount)
-   HDFS operations
-   Performance tuning using split size
-   Apache Spark DataFrame processing
-   Metadata extraction using regex
-   TF-IDF computation 
-   Cosine similarity for document similarity
-   Author Influence Network construction based on publication timelines

------------------------------------------------------------------------

#### üìà PART 1 -- Apache Hadoop & MapReduce
------------------------------------------------------------------------
#### 1Ô∏è‚É£  WordCount Execution
------------------------------------------------------------------------
#### üîç Problem Statement -

The objective of this task is to run and demonstrate the working of the WordCount example in Apache Hadoop, 
as shown on the Apache Hadoop official website and discussed in class.

#### üß© Solutions Implemented -
The requirement is:
* Execute the WordCount MapReduce program.
* Show that the program runs successfully.
* Display the output generated by Hadoop.
* No need to submit the source code ‚Äî only proof of successful execution (output) is required.

The Hadoop WordCount example was successfully executed using:

hdfs dfs -mkdir -p /lyrics/input\
hdfs dfs -put wc1.txt /lyrics/input

#### hadoop jar
\$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-\*.jar
wordcount /lyrics/input /lyrics/output

hdfs dfs -cat /lyrics/output/part-r-00000
#### üöÄ Output -
The output correctly displayed word frequencies, confirming successful
execution of the Map and Reduce phases.
  <img width="940" height="203" alt="image" src="https://github.com/user-attachments/assets/6957eac5-452e-409b-b30d-78c46a3f3dae" />


------------------------------------------------------------------------
#### 2Ô∏è‚É£ Map Phase Details
------------------------------------------------------------------------
#### üîç Problem Statement -
Suppose we run the Hadoop WordCount MapReduce program using the following song lyrics as input:<br>
  <img width="211" height="80" alt="image" src="https://github.com/user-attachments/assets/daa47571-110f-4949-8945-410fa5ef11ab" />

In Hadoop, during the Map phase, the framework provides input as key-value pairs where:
    Key ‚Üí Byte offset of the line (starting position in file)
    Value ‚Üí Entire line of text
Example input pairs to the Mapper:<br>
  <img width="250" height="80" alt="image" src="https://github.com/user-attachments/assets/88174a2e-e664-4371-8a1e-430ecd9d9850" />

We need to determine:
  What will the output key-value pairs from the Map phase look like?
  What are the data types of input and output keys and values in the Map phase?
  
#### üß© Solutions Implemented  -

#### Input to Mapper
-   Key ‚Üí LongWritable (Byte offset)
-   Value ‚Üí Text (Line of text)
Example: (0, "We're up all night till the sun")

#### Output from Mapper
-   Key ‚Üí Text (Word)
-   Value ‚Üí IntWritable (1)
Example: ("night", 1)

#### üöÄ Output -
wc1.txt <br>
  <img width="448" height="235" alt="image" src="https://github.com/user-attachments/assets/4c54f2a1-944c-4223-9921-7bcfd7fbb8c3" /><br>
  <img width="940" height="383" alt="image" src="https://github.com/user-attachments/assets/a790d63a-6a96-4067-8651-11be1c344109" />

------------------------------------------------------------------------
#### 3Ô∏è‚É£ Reduce Phase Details
------------------------------------------------------------------------

#### Input to Reducer

("up", \[1,1,1,1\])

Type: - Key ‚Üí Text - Value ‚Üí Iterable`<IntWritable>`{=html}

#### Output from Reducer

("up", 4)

Type: - Key ‚Üí Text - Value ‚Üí IntWritable

------------------------------------------------------------------------
#### 4Ô∏è‚É£ Custom WordCount Implementation
------------------------------------------------------------------------
#### üîç Problem Statement -

The objective of this task is to examine the WordCount.java file from the Hadoop MapReduce tutorial and:

- Identify the definition of the Map class and the map() method.
- Replace the placeholder data types (/*?*/) with the correct Hadoop data types.
- Identify the definition of the Reduce class and the reduce() method.
- Replace its placeholders with the correct data types.
- Determine the correct arguments to pass to:
      job.setOutputKeyClass(),
      job.setOutputValueClass()
  
#### üß© Solutions Implemented  -

Compilation:<br>
javac -classpath `hadoop classpath` WordCount.java\
jar cf wordcount.jar WordCount\*.class

#### üöÄ Output -
Execution completed without errors.<br>
<img width="940" height="491" alt="image" src="https://github.com/user-attachments/assets/fa5bdf3e-d113-4b8d-b484-3bd1365aabcf" /><br>
<img width="940" height="491" alt="image" src="https://github.com/user-attachments/assets/c2a7abee-7f87-4c82-995a-4cd6647253f1" /><br>
<img width="940" height="493" alt="image" src="https://github.com/user-attachments/assets/90d08e44-7b17-478a-bbb0-b7adb5e56563" /><br>
<img width="940" height="493" alt="image" src="https://github.com/user-attachments/assets/b2b43d0d-f62f-4798-ba9a-8d8d7939a279" /><br>
<img width="940" height="617" alt="image" src="https://github.com/user-attachments/assets/00a363df-dab3-4dd9-a468-c5aca335afca" />

------------------------------------------------------------------------
#### 5Ô∏è‚É£ HDFS Replication Concept
------------------------------------------------------------------------
#### üîç Problem Statement -
The objective of this task is to implement the map() function for the Hadoop WordCount program such that:

- Punctuation marks are ignored.
- The input line is split into individual words.
- Each word is emitted as (word, 1).
#### üß© Solutions Implemented  -
-   Implemented map() and reduce() functions\
-   Removed punctuation using replaceAll()\
-   Used StringTokenizer for tokenization
  
Directories do not have replication because: - Replication applies only
to file blocks\
- Directories store metadata only\
- Metadata is managed by NameNode

#### üöÄ Output -

<img width="940" height="522" alt="image" src="https://github.com/user-attachments/assets/85f9fc9e-b230-44cf-99ac-01da0927477e" /><br>
<img width="940" height="522" alt="image" src="https://github.com/user-attachments/assets/ea4cbc9b-3921-4bb8-b18d-407c30252b84" />

------------------------------------------------------------------------
#### 6Ô∏è‚É£ Performance Tuning -- split.maxsize
------------------------------------------------------------------------
#### üîç Problem Statement -
The objective of this task is to implement the reduce() function for the Hadoop WordCount MapReduce program.

#### üß© Solutions Implemented  -
The reduce() function must:
- Receive a word as the key.
- Receive all counts associated with that word as values.
- Sum all the counts.
- Output the final word and its total frequency.
- Ensure the project compiles successfully without any errors.

The implementation must use Hadoop data types from org.apache.hadoop.io.

#### üöÄ Output -
- The reduce() function correctly aggregates word counts.
- Hadoop data types are properly used.
- The program compiles successfully.

The WordCount MapReduce pipeline is fully implemented.
<img width="940" height="387" alt="image" src="https://github.com/user-attachments/assets/09af4c36-f303-4af7-912a-6afdc9ff828b" /><br>
<img width="940" height="510" alt="image" src="https://github.com/user-attachments/assets/d309bd22-8d46-4952-877a-e7f702d5c98b" /><br>
<img width="940" height="563" alt="image" src="https://github.com/user-attachments/assets/304afd3c-7869-42ee-9459-3f0cfad76d0d" /><br>
<img width="940" height="380" alt="image" src="https://github.com/user-attachments/assets/aff7e5b1-3a20-495c-aa83-1a9f308354b2" />

------------------------------------------------------------------------
#### 7Ô∏è‚É£ Executing WordCount on 200.txt in Hadoop Cluster
------------------------------------------------------------------------
#### üîç Problem Statement -
The objective of this task is to run the WordCount MapReduce program on the file 200.txt in a Hadoop cluster environment.

The process involves:
- Copying 200.txt from the local system to HDFS.
- Executing the compiled WordCount.jar file on the cluster.
- Verifying successful execution.
- Merging the distributed output files into a single local file.
- Checking whether the output is correctly generated in the expected format (e.g., A 182, AA 16, AAN 5, etc.).
#### üß© Solutions Implemented  -

- Copy 200.txt to HDFS
- Remove Previous Output
- Run WordCount Program
- Merge Output to Local File

#### üöÄ Output -
<img width="940" height="431" alt="image" src="https://github.com/user-attachments/assets/3052446c-f9d4-43b5-bef4-c24614853c36" />

------------------------------------------------------------------------
#### 8Ô∏è‚É£ HDFS File Operations and Replication Mechanism
------------------------------------------------------------------------
#### üîç Problem Statement -
The objective of this task is to understand how file system operations work in Hadoop Distributed File System (HDFS) compared to a standard UNIX file system.

- Explain why files have a replication factor in HDFS, but directories do not.
  
------------------------------------------------------------------------
#### 9Ô∏è‚É£ Executing WordCount on 200.txt in Hadoop Cluster
------------------------------------------------------------------------
#### üîç Problem Statement -

The objective of this task is to modify the WordCount.java program to:
- Measure and display the total execution time of the MapReduce job.
- Experiment with the configuration parameter mapreduce.input.fileinputformat.split.maxsize.
- Analyze how changing the value of split.maxsize affects job performance.
- Explain why performance changes when the split size is modified.

#### üß© Solutions Implemented  -
- Measuring Total Execution Time
- Experimenting with split.maxsize
- Why Performance Changes

#### üöÄ Output -


# üü¢ PART 2 -- Apache Spark

Dataset: Project Gutenberg books

Schema:

file_name (string)\
text (string)

------------------------------------------------------------------------

# üìò Metadata Extraction

Regex Used:

-   Title ‚Üí (?i)Title:`\s*`{=tex}(.+)\
-   Release Date ‚Üí (?i)Release Date:`\s*`{=tex}(.+)\
-   Language ‚Üí (?i)Language:`\s*`{=tex}(.+)\
-   Encoding ‚Üí (?i)Character set encoding:`\s*`{=tex}(.+)

Extracted using Spark regexp_extract().

------------------------------------------------------------------------

# üìä Analysis Performed

-   Books released per year\
-   Most common language\
-   Average title length

Challenges: - Missing metadata\
- Inconsistent formatting\
- Noise in text

------------------------------------------------------------------------

# üìò TF-IDF & Book Similarity

## Term Frequency (TF)

Measures frequency of word in document.

## Inverse Document Frequency (IDF)

IDF = log(N / df)

## TF-IDF

TF-IDF = TF √ó IDF

Highlights distinguishing words and reduces importance of common words.

------------------------------------------------------------------------

## Cosine Similarity

Cosine(A,B) = (A¬∑B) / (\|\|A\|\| \|\|B\|\|)

-   1 ‚Üí Highly similar\
-   0 ‚Üí Not similar

Suitable because it normalizes document length and works well with
sparse vectors.

------------------------------------------------------------------------

# üìò Author Influence Network

Definition: Author A influences Author B if: 0 \< (year_B - year_A) ‚â§ X

Represented as Spark DataFrame of directed edges: (author1, author2)

Computed: - In-degree\
- Out-degree

------------------------------------------------------------------------

# üõ† Technologies Used

-   Apache Hadoop\
-   HDFS\
-   Java\
-   Apache Spark\
-   PySpark\
-   Spark MLlib\
-   Regular Expressions\
-   TF-IDF\
-   Cosine Similarity

------------------------------------------------------------------------

# üìå Conclusion

This project demonstrates distributed computing fundamentals using
Hadoop and scalable analytics using Apache Spark. It includes MapReduce
implementation, performance tuning, metadata extraction, document
similarity analysis, and influence network construction.

