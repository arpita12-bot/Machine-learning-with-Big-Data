# Apache Hadoop & Apache Spark 

 Assignment 1 -- MapReduce and Spark Analysis

**Name:** Arpita Kundu\
**Roll No:** M25DE1004

------------------------------------------------------------------------

### üìå Project Overview

This assignment demonstrates practical implementation of:

-   Apache Hadoop MapReduce (WordCount)
-   HDFS operations
-   Performance tuning using split size
-   Apache Spark DataFrame processing
-   Metadata extraction using regex
-   TF-IDF computation 
-   Cosine similarity for document similarity
-   Author Influence Network construction based on publication timelines

------------------------------------------------------------------------

#### üìà PART 1 -- Apache Hadoop & MapReduce
------------------------------------------------------------------------
#### 1Ô∏è‚É£  WordCount Execution
------------------------------------------------------------------------
#### üîç Problem Statement -

The objective of this task is to run and demonstrate the working of the WordCount example in Apache Hadoop, 
as shown on the Apache Hadoop official website.

#### üß© Solutions Implemented -
The requirement is:
* Execute the WordCount MapReduce program.
* Show that the program runs successfully.
* Display the output generated by Hadoop.

The Hadoop WordCount example was successfully executed using:

hdfs dfs -mkdir -p /lyrics/input\
hdfs dfs -put wc1.txt /lyrics/input

#### hadoop jar
\$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-\*.jar
wordcount /lyrics/input /lyrics/output

hdfs dfs -cat /lyrics/output/part-r-00000
#### üöÄ Output -
The output correctly displayed word frequencies, confirming successful
execution of the Map and Reduce phases.
  <img width="940" height="203" alt="image" src="https://github.com/user-attachments/assets/6957eac5-452e-409b-b30d-78c46a3f3dae" />


------------------------------------------------------------------------
#### 2Ô∏è‚É£ Map Phase Details
------------------------------------------------------------------------
#### üîç Problem Statement -
Suppose we run the Hadoop WordCount MapReduce program using the following song lyrics as input:<br>
  <img width="211" height="80" alt="image" src="https://github.com/user-attachments/assets/daa47571-110f-4949-8945-410fa5ef11ab" />

In Hadoop, during the Map phase, the framework provides input as key-value pairs where:
    Key ‚Üí Byte offset of the line (starting position in file)
    Value ‚Üí Entire line of text
Example input pairs to the Mapper:<br>
  <img width="250" height="80" alt="image" src="https://github.com/user-attachments/assets/88174a2e-e664-4371-8a1e-430ecd9d9850" />

We need to determine:
  What will the output key-value pairs from the Map phase look like?
  What are the data types of input and output keys and values in the Map phase?
  
#### üß© Solutions Implemented  -

#### Input to Mapper
-   Key ‚Üí LongWritable (Byte offset)
-   Value ‚Üí Text (Line of text)
Example: (0, "We're up all night till the sun")

#### Output from Mapper
-   Key ‚Üí Text (Word)
-   Value ‚Üí IntWritable (1)
Example: ("night", 1)

#### üöÄ Output -
wc1.txt <br>
  <img width="448" height="235" alt="image" src="https://github.com/user-attachments/assets/4c54f2a1-944c-4223-9921-7bcfd7fbb8c3" /><br>
  <img width="940" height="383" alt="image" src="https://github.com/user-attachments/assets/a790d63a-6a96-4067-8651-11be1c344109" />

------------------------------------------------------------------------
#### 3Ô∏è‚É£ Reduce Phase Details
------------------------------------------------------------------------

#### Input to Reducer

("up", \[1,1,1,1\])

Type: - Key ‚Üí Text - Value ‚Üí Iterable`<IntWritable>`{=html}

#### Output from Reducer

("up", 4)

Type: - Key ‚Üí Text - Value ‚Üí IntWritable

------------------------------------------------------------------------
#### 4Ô∏è‚É£ Custom WordCount Implementation
------------------------------------------------------------------------
#### üîç Problem Statement -

The objective of this task is to examine the WordCount.java file from the Hadoop MapReduce tutorial and:

- Identify the definition of the Map class and the map() method.
- Replace the placeholder data types (/*?*/) with the correct Hadoop data types.
- Identify the definition of the Reduce class and the reduce() method.
- Replace its placeholders with the correct data types.
- Determine the correct arguments to pass to:
      job.setOutputKeyClass(),
      job.setOutputValueClass()
  
#### üß© Solutions Implemented  -

Compilation:<br>
javac -classpath `hadoop classpath` WordCount.java\
jar cf wordcount.jar WordCount\*.class

#### üöÄ Output -
Execution completed without errors.<br>
<img width="940" height="491" alt="image" src="https://github.com/user-attachments/assets/fa5bdf3e-d113-4b8d-b484-3bd1365aabcf" /><br>
<img width="940" height="491" alt="image" src="https://github.com/user-attachments/assets/c2a7abee-7f87-4c82-995a-4cd6647253f1" /><br>
<img width="940" height="493" alt="image" src="https://github.com/user-attachments/assets/90d08e44-7b17-478a-bbb0-b7adb5e56563" /><br>
<img width="940" height="493" alt="image" src="https://github.com/user-attachments/assets/b2b43d0d-f62f-4798-ba9a-8d8d7939a279" /><br>
<img width="940" height="617" alt="image" src="https://github.com/user-attachments/assets/00a363df-dab3-4dd9-a468-c5aca335afca" />

------------------------------------------------------------------------
#### 5Ô∏è‚É£ HDFS Replication Concept
------------------------------------------------------------------------
#### üîç Problem Statement -
The objective of this task is to implement the map() function for the Hadoop WordCount program such that:

- Punctuation marks are ignored.
- The input line is split into individual words.
- Each word is emitted as (word, 1).
#### üß© Solutions Implemented  -
-   Implemented map() and reduce() functions\
-   Removed punctuation using replaceAll()\
-   Used StringTokenizer for tokenization
  
Directories do not have replication because: - Replication applies only
to file blocks\
- Directories store metadata only\
- Metadata is managed by NameNode

#### üöÄ Output -

<img width="940" height="522" alt="image" src="https://github.com/user-attachments/assets/85f9fc9e-b230-44cf-99ac-01da0927477e" /><br>
<img width="940" height="522" alt="image" src="https://github.com/user-attachments/assets/ea4cbc9b-3921-4bb8-b18d-407c30252b84" />

------------------------------------------------------------------------
#### 6Ô∏è‚É£ Performance Tuning -- split.maxsize
------------------------------------------------------------------------
#### üîç Problem Statement -
The objective of this task is to implement the reduce() function for the Hadoop WordCount MapReduce program.

#### üß© Solutions Implemented  -
The reduce() function must:
- Receive a word as the key.
- Receive all counts associated with that word as values.
- Sum all the counts.
- Output the final word and its total frequency.
- Ensure the project compiles successfully without any errors.

The implementation must use Hadoop data types from org.apache.hadoop.io.

#### üöÄ Output -
- The reduce() function correctly aggregates word counts.
- Hadoop data types are properly used.
- The program compiles successfully.

The WordCount MapReduce pipeline is fully implemented.
<img width="940" height="387" alt="image" src="https://github.com/user-attachments/assets/09af4c36-f303-4af7-912a-6afdc9ff828b" /><br>
<img width="940" height="510" alt="image" src="https://github.com/user-attachments/assets/d309bd22-8d46-4952-877a-e7f702d5c98b" /><br>
<img width="940" height="563" alt="image" src="https://github.com/user-attachments/assets/304afd3c-7869-42ee-9459-3f0cfad76d0d" /><br>
<img width="940" height="380" alt="image" src="https://github.com/user-attachments/assets/aff7e5b1-3a20-495c-aa83-1a9f308354b2" />

------------------------------------------------------------------------
#### 7Ô∏è‚É£ Executing WordCount on 200.txt in Hadoop Cluster
------------------------------------------------------------------------
#### üîç Problem Statement -
The objective of this task is to run the WordCount MapReduce program on the file 200.txt in a Hadoop cluster environment.

The process involves:
- Copying 200.txt from the local system to HDFS.
- Executing the compiled WordCount.jar file on the cluster.
- Verifying successful execution.
- Merging the distributed output files into a single local file.
- Checking whether the output is correctly generated in the expected format (e.g., A 182, AA 16, AAN 5, etc.).
#### üß© Solutions Implemented  -

- Copy 200.txt to HDFS
- Remove Previous Output
- Run WordCount Program
- Merge Output to Local File

#### üöÄ Output -
<img width="940" height="536" alt="image" src="https://github.com/user-attachments/assets/d232a8fc-e036-4fb0-8b87-48f609f706f4" />


------------------------------------------------------------------------
#### 8Ô∏è‚É£ HDFS File Operations and Replication Mechanism
------------------------------------------------------------------------
#### üîç Problem Statement -
The objective of this task is to understand how file system operations work in Hadoop Distributed File System (HDFS) compared to a standard UNIX file system.

- Explain why files have a replication factor in HDFS, but directories do not.
  
------------------------------------------------------------------------
#### 9Ô∏è‚É£ Executing WordCount on 200.txt in Hadoop Cluster
------------------------------------------------------------------------
#### üîç Problem Statement -

The objective of this task is to modify the WordCount.java program to:
- Measure and display the total execution time of the MapReduce job.
- Experiment with the configuration parameter mapreduce.input.fileinputformat.split.maxsize.
- Analyze how changing the value of split.maxsize affects job performance.
- Explain why performance changes when the split size is modified.

#### üß© Solutions Implemented  -
- Measuring Total Execution Time
- Experimenting with split.maxsize
- Why Performance Changes

#### üöÄ Output -
<img width="940" height="495" alt="image" src="https://github.com/user-attachments/assets/8c387e09-6398-4063-8058-1e4bf0b997a7" /><br>
<img width="940" height="491" alt="image" src="https://github.com/user-attachments/assets/6a3740cc-73af-4413-ad2d-29972859e1e5" />
------------------------------------------------------------------------
# üü¢ PART 2 -- Apache Spark(PySpark)
------------------------------------------------------------------------

Dataset: Project Gutenberg books
Schema:
 - file_name (string)\
 - text (string)
------------------------------------------------------------------------

# üìò Metadata Extraction

Regex Used:

-   Title ‚Üí (?i)Title:`\s*`{=tex}(.+)\
-   Release Date ‚Üí (?i)Release Date:`\s*`{=tex}(.+)\
-   Language ‚Üí (?i)Language:`\s*`{=tex}(.+)\
-   Encoding ‚Üí (?i)Character set encoding:`\s*`{=tex}(.+)

Extracted using Spark regexp_extract().
------------------------------------------------------------------------
#### üìò Book Metadata Extraction and Analysis
------------------------------------------------------------------------
#### üîç Problem Statement -

Extracts metadata from Project Gutenberg files and performs statistical analysis:
- Books per year: Distribution of books by release year (1500-2026)
- Language analysis: Most common languages in the corpus
- Title statistics: Average title length
- Validates data quality by identifying missing or invalid release years

#### üìä Analysis Performed

-   Books released per year
-   Most common language
-   Average title length

Challenges: - Missing metadata
- Inconsistent formatting
- Noise in text


- Highlights distinguishing words and reduces importance of common words.
- Explain the regular expressions you used to extract the title, release date, language, and
encoding. Discuss any challenges or limitations in using regular expressions for this task.
- What are some potential issues with the extracted metadata (e.g., inconsistencies,
missing values)? How would you handle these issues in a real-world scenario?

#### üöÄ Output -

<img width="940" height="178" alt="image" src="https://github.com/user-attachments/assets/256a0da7-a685-4273-a32b-14aed966e581" /><br>
Count books per year:<br>
<img width="591" height="781" alt="image" src="https://github.com/user-attachments/assets/631a8456-00df-4c1a-bb82-823937a24928" /><br>
Most common language in the dataset:<br>
<img width="645" height="409" alt="image" src="https://github.com/user-attachments/assets/562c6a1e-4c39-42d1-816d-066645fb1994" /><br>
Average length of book titles (characters):<br>
<img width="940" height="298" alt="image" src="https://github.com/user-attachments/assets/f0b81c86-5397-44c3-8526-ab4ff02b78ec" /><br>



------------------------------------------------------------------------
#### üìò TF-IDF and Book Similarity
------------------------------------------------------------------------
#### üîç Problem Statement -
Implements text similarity analysis using Term Frequency-Inverse Document Frequency:

- Cleans text (removes Gutenberg headers/footers, punctuation)
- Tokenizes and removes stopwords using Spark ML
- Computes TF-IDF vectors for each document
- Calculates cosine similarity between book pairs
- Identifies the top 5 most similar books to a target document (default: 10.txt)

------------------------------------------------------------------------
#### üìò Author Influence Network
------------------------------------------------------------------------
#### üîç Problem Statement -
Constructs a directed graph representing potential author influences:

- Edge definition: Author A ‚Üí Author B if B published within X years after A (default X=5)
- Graph metrics:
- Out-degree: Number of authors potentially influenced by this author
- In-degree: Number of authors who potentially influenced this author
- Identifies most influential and most influenced authors
  
#### üöÄ Output -
<img width="940" height="232" alt="image" src="https://github.com/user-attachments/assets/902622f4-417d-4da7-952d-bb043abbea51" /><br>
Extract Author and Release Year:<br>
<img width="940" height="402" alt="image" src="https://github.com/user-attachments/assets/f6b29d0c-f80b-475e-bec3-bc467e7e7e4f" /><br>
Extract Release Year:<br>
<img width="940" height="553" alt="image" src="https://github.com/user-attachments/assets/8bf546e2-d25b-4a48-b87b-416030e64a47" /><br>
<img width="679" height="832" alt="image" src="https://github.com/user-attachments/assets/33c912f7-c83c-4225-8b83-ebed217bcdee" /><br>

 Construct Influence Network:<br>
 <img width="592" height="839" alt="image" src="https://github.com/user-attachments/assets/19e39acf-ed7f-440d-b00c-a0c050df7f3e" /><br>

 Compute In-Degree and Out-Degree:<br>
 <img width="757" height="873" alt="image" src="https://github.com/user-attachments/assets/22ab5315-ffa9-403d-ae17-f7b984194263" /><br>
 <img width="767" height="860" alt="image" src="https://github.com/user-attachments/assets/971875e9-91ec-4857-bd63-d68c27eb4780" /><br>

 Top5:<br>
 <img width="914" height="344" alt="image" src="https://github.com/user-attachments/assets/d85473aa-7ab6-45c7-bcb3-ca305967e3ec" /><br>
 <img width="903" height="345" alt="image" src="https://github.com/user-attachments/assets/b5117f01-2840-4e95-95d0-f163359090f7" />


------------------------------------------------------------------------
#### üõ† Technologies Used
------------------------------------------------------------------------
- Apache Hadoop(HDFS,Java)
- Apache Spark(PySpark)
- Unix set up

------------------------------------------------------------------------
#### SetUp
------------------------------------------------------------------------
#### Apache Hadoop & MapReduce
- Install Java (JDK 8 or 11).
- Download Apache Hadoop.
- Extract and configure:
   1. core-site.xml
   2. hdfs-site.xml
   3. mapred-site.xml
   4. yarn-site.xml
#### Apache Spark
- Download Pyspark(Apache spark)
- Extract and set environment variables:
  
------------------------------------------------------------------------
#### Project Structure
------------------------------------------------------------------------




#### üìå Conclusion

This project demonstrates distributed computing fundamentals using
Hadoop and scalable analytics using Apache Spark. It includes MapReduce
implementation, performance tuning, metadata extraction, document
similarity analysis, and influence network construction.



Submitted By,
- M25DE1004(Arpita Kundu) | Indian Institute Of Technology Jodhpur(IITJ)

